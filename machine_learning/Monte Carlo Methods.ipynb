{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook introduces the basics of Monte Carlo methods for numerically solving problems that may be intractable analytically. In general, our goal will be to solve problems in expectation, using the law of total expectation:\n",
    "\n",
    "$\\mathbf{E}_{x \\sim p}[f(x)] = \\int_{x}p(x)f(x)dx$\n",
    "\n",
    "Variants of this type of problem are fairly common in, for example, reinforcement learning and machine learning, where we estimate and minimize some expected cost function. Further applications exist in areas such as finance, where the goal is to model returns on investment portfolios. For many interesting problems, finding an analytical solution is either impossible, or far more difficult than approximating the solution numerically.\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    "1. We first introduce the ideas of expected values and integration.\n",
    "2. We demonstrate methods for calculating the expected value of a function $f(x)$.\n",
    "3. We show methods for estimating the value of the integral of $f(x)$ on the interval $[a,b]$.\n",
    "4. We show methods for calculating the gradient of $f(x)$ at a point of interest.\n",
    "5. We go through methods of variance reduction that can help make our estimates more computationally feasible and accurate for a larger set of problems.\n",
    "\n",
    "Throughout, each example will be accompanied with code that can be run independent of the other examples. This code will show a very basic implementation of the idea in question, written in numpy/scipy. Variance reduction methods will come with associated code to show that they do, indeed, reduce variance. The implementation is intended to allow students to interrogate the code in order to figure out under what conditions these methods work.\n",
    "\n",
    "\n",
    "## 2. Expected Values and Integration\n",
    "\n",
    "Equation 1 above gives the general form of the expected value of a function, which lets us estimate the expected value of the function using:\n",
    "\n",
    "$\\mathbf{E}_{x \\sim p}[f(x)] = \\int_{x}p(x)f(x)dx \\approx \\frac{1}{N}\\sum_i^{N}f(x_i)$\n",
    "\n",
    "To see that this is true, we can approximate the integral of a function $f(x)$ as being: \n",
    "\n",
    "$\\int_{x}p(x)f(x)dx \\approx \\sum_i p(x_i)f(x_i) \\Delta x$ \n",
    "\n",
    "where $\\Delta x$ is a fixed step size (i.e. this is a Riemann sum). Assuming a fixed interval $[a, b]$ over which we're integrating, we can choose a number of \"slices\" for the Riemann sum, and calculate the step size using:\n",
    "\n",
    "$\\Delta x = \\frac{(a-b)}{N}$\n",
    "\n",
    "Which, plugging back into our original equation, gives:\n",
    "\n",
    "$\\int_{x}p(x)f(x)dx \\approx \\sum_i p(x_i)f(x_i) \\Delta x = \\sum_i p(x_i)f(x_i) \\frac{(a-b)}{N}$\n",
    "\n",
    "If $p(x)$ is a uniform distribution on $[a, b]$, then $(a-b) = \\frac{1}{p(x)}$, leaving $\\int_{x}p(x)f(x)dx \\approx \\sum_i f(x_i) \\frac{1}{N} = \\frac{1}{N}\\sum_i^{N}f(x_i)$. If $p(x)$ isn't a uniform distribution, we can call $(a-b) = \\frac{1}{q(x)}$, which gives us a weighted average of the function:\n",
    "\n",
    "$\\int_{x}p(x)f(x)dx \\approx \\sum_i p(x_i)f(x_i) \\Delta x = \\frac{1}{N}\\sum_i \\frac{p(x_i)}{q(x_i)}f(x_i)$\n",
    "\n",
    "Note that in this case, we have chosen our step size as being distributed by $q(x)$, so the expectation is now:\n",
    "\n",
    "$\\frac{1}{N}\\sum_i \\frac{p(x_i)}{q(x_i)}f(x_i) \\approx \\mathbf{E}_{x \\sim q}\\left[\\frac{p(x_i)}{q(x_i)}f(x)\\right] = \\int_x q(x) \\frac{p(x)}{q(x)} f(x) dx$\n",
    "\n",
    "This is known as importance sampling, and can be useful in cases where the events that we're interested in are very rare under $p(x)$. Though we've used it with a uniform distribution above, the technique is general, and works with any distribution (though for definite integrals, we will usually use a uniform distribution somewhere to assign zero probability to samples outside of the interval that we're integrating over). More generally, we can propose a new distribution $q(x)$ to make the events we're interested in more frequent, but we need to correct for the fact that we've now introduced bias into our sampling procedure. \n",
    "\n",
    "Finally, these ideas imply that we can use the expected value over an interval to calculate an integral. To show this:\n",
    "\n",
    "$\\int_x f(x) dx \\approx \\sum_i f(x_i) \\frac{(b - a)}{N} = \\frac{\\mathbf{E}_{x \\sim p} \\left[f(x)\\right]}{p(x)}$\n",
    "\n",
    "For $x \\sim \\mathcal{U}(a, b)$. Similarly for a multivariate function:\n",
    "\n",
    "$\\int_y\\int_x f(x, y) dx dy \\approx \\sum_i \\sum_j f(x_j, y_i) \\Delta x \\Delta y$\n",
    "\n",
    "$\\sum_i \\sum_j f(x_j, y_i) \\Delta x \\Delta y = \\frac{(d - c)}{N_y}\\sum_i \\frac{(b - a)}{N_x}\\sum_j f(x_j, y_i)$\n",
    "\n",
    "$\\frac{(d - c)}{N_y}\\sum_i \\frac{(b - a)}{N_x}\\sum_j f(x_j, y_i) = \\frac{\\mathbf{E}_{y \\sim q}\\left[\\mathbf{E}_{x\\sim p}\\left[f(x,y)|y\\right]\\right]}{p(x)q(y)} = \\frac{\\mathbf{E}_{x\\sim p, y \\sim q}\\left[f(x,y)\\right]}{p(x)q(y)}$\n",
    "\n",
    "For $x \\sim \\mathcal{U}(a, b)$, $y \\sim \\mathcal{U}(c, d)$ (though the expectations hold more generally). The final equality suggests that we can evaluate a multivariate integral using only a single Monte Carlo integral by sampling $x$ and $y$ from $p(x)$ and $q(y)$ respectively, evaluating the function $f(x,y)$ at those points, taking the expectation, and then dividing by $p(x)q(y)$. This logic holds for any number of dimensions. One thing to be careful of is that as the dimensionality of the problem increases, so too will the number of samples required for integration; the relative sparsity of N points on the interval $[a, b]$ will be lower than for the area $[a,b]\\times[c,d]$, and so on. We could also sample from a joint distribution $p(x,y)$, and use this to integrate over the interval (an example of this using copulae is demonstrated further on).\n",
    "\n",
    "To illustrate these ideas, see the code below:\n",
    "\n",
    "### 2.1 Calculating Expected Values and Integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected value:  12.1209713048\n",
      "integral value:  72.7258278287\n",
      "\n",
      "expected value:  12.018005999999813\n",
      "integral value:  72.10803599999888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "\n",
    "# we will integrate the function f = x^2 in the interval [a,b]\n",
    "a = 0\n",
    "b = 6\n",
    "samples = 1000\n",
    "x = np.random.uniform(low=a, high=b, size=samples)\n",
    "y = f(x)\n",
    "expected = np.mean(y)\n",
    "print(\"expected value: \", expected)\n",
    "print(\"integral value: \", expected * (b - a))\n",
    "\n",
    "\n",
    "# next, we will do numerical integration over the same interval to calculate the value\n",
    "dx = (b - a) / samples\n",
    "integral = 0\n",
    "x = 0\n",
    "for i in range(samples + 1):\n",
    "    integral += f(x) * dx\n",
    "    x += dx\n",
    "print()\n",
    "print(\"expected value: \", integral / (b - a))\n",
    "print(\"integral value: \", integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected value:  12.0397040986\n",
      "integral value:  72.2382245916\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, pi\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "normal_pdf = lambda x, mu, sigma: 1/(sigma * sqrt(2 * pi)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "uniform_pdf = lambda xs, a, b: np.array([1/(b - a) if x < b and x > a else 0 for x in xs])\n",
    "\n",
    "a = 0\n",
    "b = 6\n",
    "samples = 10000\n",
    "mu = 0.5\n",
    "sigma = 15\n",
    "x = np.random.normal(mu, sigma, size=samples)\n",
    "y = f(x)\n",
    "w = uniform_pdf(x, a, b) / normal_pdf(x, mu, sigma)\n",
    "expected = np.mean(w * y)\n",
    "print(\"expected value: \", expected)\n",
    "print(\"integral value: \", expected * (b - a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get a similar result to the previous example. One thing to be careful of is that our biased sampling can improve our integral estimate, but it can also make it worse if the distribution we choose has poor (or no) support on the interval that we're integrating over. For example, the $\\sigma$ for the distribution $q(x)$ above is 15 in order to ensure that our samples have good coverage of the interval, and our uniform PDF assigns zero probability to anything outside of the interval. This means that we need to take many more samples to get a decent approximation of the functions we're interested in. What happens if you reduce the standard deviation of $q$ to a small value? What happens if you make $q(x)$ uniform with support greater than that of $p(x)$? What happens if the interval of $q(x)$ doesn't cover $p(x)$?\n",
    "\n",
    "### 2.3 Self-Normalized Importance Sampling\n",
    "\n",
    "In many cases, we want to sample from a distribution, but we only know it up to a normalizing constant. I.e. our distribution is:\n",
    "\n",
    "$p(x) = \\frac{g(x)}{\\int_x g(x) dx} = \\frac{g(x)}{Z}$\n",
    "\n",
    "Say that we have two unnormalized distributions, $g(x)$, as above, and $h(x)$. If we're lucky, and $Z_g = Z_h$, then we can use standard importance sampling as above, since the constants cancel:\n",
    "\n",
    "$\\frac{p(x)}{q(x)} = \\frac{Z_h g(x)}{Z_g h(x)} = \\frac{g(x)}{h(x)}$\n",
    "\n",
    "In cases where the normalizing constants are different, we need to utilize self-normalized importance sampling:\n",
    "\n",
    "$\\int_x p(x)f(x)dx = \\int_x \\frac{p(x)}{q(x)}f(x)q(x)dx$\n",
    "\n",
    "$\\int_x p(x)f(x)dx = \\frac{\\int_x \\frac{p(x)}{q(x)}f(x)q(x)dx}{\\int_x \\frac{p(x)}{q(x)}q(x)dx}$\n",
    "\n",
    "Since ${\\int_x \\frac{p(x)}{q(x)}q(x)dx} = 1$. Then:\n",
    "\n",
    "$\\int_x p(x)f(x)dx = \\frac{\\int_x \\frac{g(x)}{h(x)}f(x)q(x)dx}{\\int_x \\frac{g(x)}{h(x)}q(x)dx}$\n",
    "\n",
    "$\\int_x p(x)f(x)dx = \\frac{\\mathbf{E}\\left[\\frac{g(x)}{h(x)}f(x)\\right]}{\\mathbf{E}\\left[\\frac{g(x)}{h(x)}\\right]}$\n",
    "\n",
    "In this way we can still get an estimate of $f(x)$ even without normalized distributions. For an example in code, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self weighted import sampling estimate:  11.3132085311\n",
      "Standard estimate:  11.945458935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, pi\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "normal_pdf = lambda x, mu, sigma: 1/(sigma * sqrt(2 * pi)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "uniform_pdf = lambda xs, a, b: np.array([1/(b - a) if x < b and x > a else 0 for x in xs])\n",
    "\n",
    "# build unnormalized distributions and calc self-weighted importance sampling estimate\n",
    "Z_g = 2\n",
    "Z_h = 3\n",
    "a = 0\n",
    "b = 6\n",
    "samples = 10000\n",
    "mu = (b - a) / 2\n",
    "sigma = 15\n",
    "x = np.random.normal(mu, sigma, size=samples)\n",
    "g_x = Z_g * uniform_pdf(x, a, b)\n",
    "h_x = Z_h * normal_pdf(x, mu, sigma)\n",
    "y = f(x)\n",
    "w = g_x / h_x\n",
    "\n",
    "# calculate standard estimate\n",
    "x_ = np.random.uniform(a, b, size=samples)\n",
    "y_ = f(x_)\n",
    "expected = np.mean(w * y)/np.mean(w)\n",
    "print(\"Self weighted import sampling estimate: \", expected)\n",
    "print(\"Standard estimate: \", np.mean(y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients\n",
    "\n",
    "So now we know how to calculate the average value of a function $f(x)$ over an interval, and its integral over that interval. What if we want to calculate its gradient at a point? There are a few ways we can do this using Monte Carlo methods. The first and simplest is known as the score function:\n",
    "\n",
    "### 3.1 Score Function Estimator\n",
    "\n",
    "Let:\n",
    "\n",
    "$\\nabla\\mathbf{E}_{x \\sim p}[f(x)] = \\nabla\\int_{x}p(x)f(x)dx \\approx \\nabla\\frac{1}{N}\\sum_i^{N}f(x_i)$\n",
    "\n",
    "We can't take the gradient with respect to $x$ in the above case, since that undoes the integral (and hence the expectation). What we can do instead is parameterize $p(x)$ as a function of a new parameter $\\theta$, such that we now have $p_{\\theta}(x)$. Next, we're going to take the gradient of our function with respect to $\\theta$, which lets us move it under the integral sign:\n",
    "\n",
    "$\\nabla_{\\theta}\\mathbf{E}_{x \\sim p}[f(\\theta)] = \\int_{x}\\nabla_{\\theta}p_{\\theta}(x)f(x)dx \\approx \\frac{1}{N}\\sum_i^{N} \\nabla_{\\theta}f(x_i)$\n",
    "\n",
    "Notice that we can now introduce a clever identity: \n",
    "\n",
    "$\\nabla_{\\theta}p_{\\theta}(x) = p_{\\theta}(x)\\frac{\\nabla_{\\theta}p_{\\theta}(x)}{p_{\\theta}(x)} = p_{\\theta}(x)\\nabla_{\\theta}\\log{p_{\\theta}(x)}$\n",
    "\n",
    "Subbing this back into the above equation, we get:\n",
    "\n",
    "$\\nabla_{\\theta}\\mathbf{E}_{x \\sim p}[f(\\theta)] = \\int_{x}p_{\\theta}(x)\\nabla_{\\theta}\\log{p_{\\theta}(x)}f(x)dx \\approx \\mathbf{E}_{x \\sim p}\\left[\\nabla_{\\theta}\\log{p_{\\theta}(x)f(x)}\\right]$\n",
    "\n",
    "To see how we can use this to calculate the gradient of a function $f(x)$ at a point $x=\\theta$, we do a quick example below:\n",
    "\n",
    "Let $f(x) = x^2$, and $x \\sim \\mathcal{N}(\\theta, \\sigma)$. Then:\n",
    "\n",
    "$p_{\\theta}(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-0.5\\left(\\frac{x - \\theta}{\\sigma}\\right)^2}$\n",
    "\n",
    "$\\log{p_{\\theta}(x)} = -\\log{K}-0.5\\left(\\frac{x - \\theta}{\\sigma}\\right)^2$\n",
    "\n",
    "$\\nabla_{\\theta}\\log{p_{\\theta}(x)} = \\frac{1}{\\sigma^2}\\left(x - \\theta\\right)$\n",
    "\n",
    "$\\mathbf{E}_{x\\sim p}\\left[\\nabla_{\\theta}\\log{p_{\\theta}(x)}f(x)\\right] \\approx \\frac{1}{\\sigma^2 N}\\sum_i\\left(x_i - \\theta\\right)f(x_i)$\n",
    "\n",
    "If $\\sigma = 1$, this simplifies to $\\frac{1}{N}\\sum_i\\left(x_i - \\theta\\right)f(x_i)$.\n",
    "\n",
    "To see this working in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(x) at point 3.00: 6.19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi, log\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "grad_log_prob = lambda theta, sigma, x: (x - theta) / sigma**2\n",
    "\n",
    "samples = 1000\n",
    "theta = 3\n",
    "sigma = 1\n",
    "x = np.random.normal(theta, sigma, samples)\n",
    "glp = grad_log_prob(theta, sigma, x)\n",
    "grad_f = np.mean(glp * f(x))\n",
    "print(\"gradient of f(x) at point {:.2f}: {:.2f}\".format(theta, grad_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this works to calculate the gradient at the point where $\\theta$ is our point of interest.\n",
    "\n",
    "We can also combine this idea with importance sampling as outlined above. Essentially:\n",
    "\n",
    "$\\nabla_{\\theta}\\mathbf{E}_{x \\sim p}[f(\\theta)] = \\int_{x}\\frac{q(x)}{q(x)}p_{\\theta}(x)\\nabla_{\\theta}\\log{p_{\\theta}(x)}f(x)dx \\approx \\mathbf{E}_{x \\sim q}\\left[\\frac{p_{\\theta}(x)}{q(x)}\\nabla_\\theta\\log{p_{\\theta}(x)f(x)}\\right]$\n",
    "\n",
    "To demonstrate this point, we implement the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(x) at point 3.00: 6.22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi, log, sqrt\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "normal_pdf = lambda x, mu, sigma: 1/(sigma * sqrt(2 * pi)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "grad_log_prob = lambda theta, sigma, x: (x - theta) / sigma**2\n",
    "\n",
    "samples = 10000\n",
    "theta = 3\n",
    "\n",
    "mu_q = 0.5\n",
    "sigma_q = 15\n",
    "\n",
    "mu_p = theta\n",
    "sigma_p = 2\n",
    "\n",
    "x = np.random.normal(mu_q, sigma_q, samples)\n",
    "glp = grad_log_prob(theta, sigma_p, x)\n",
    "w = normal_pdf(x, mu_p, sigma_p) / normal_pdf(x, mu_q, sigma_q)\n",
    "grad_f = np.mean(w * glp * f(x))\n",
    "print(\"gradient of f(x) at point {:.2f}: {:.2f}\".format(theta, grad_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score function method is the workhorse of REINFORCE-based policy search methods in reinforcement learning. Typically, we parameterize a neural network that outputs a mean and log-variance. Actions are sampled from this distribution, and the \"value\" of the policy is assessed using its return. The score function estimator is then used to step the policy weights in the direction that maximizes the return using gradient ascent.\n",
    "\n",
    "### 3.2 Change of Variables (Reparameterization Trick)\n",
    "\n",
    "There's an alternative method for calculating the gradient that results from the Change of Variables theorem. An equivalent representation to the score function estimator is:\n",
    "\n",
    "$\\int_{x}p_\\theta(x)\\nabla_{\\theta}\\log{p_{\\theta}(x)}f(x)dx = \\int_{\\epsilon}\\nabla_{x}f(g(\\theta; \\epsilon))\\nabla_{\\theta}g(\\theta; \\epsilon)p(\\epsilon)d\\epsilon$\n",
    "\n",
    "That is, we make $x$ a function of $\\theta$, such that $x = g(\\theta) = \\theta + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$, and $p(x)dx = p(\\epsilon)d\\epsilon$. This estimator typically shows lower variance than the score function estimator, which is beneficial, as it means we don't have to draw as many samples to get  good estimate. To show a concrete example of this estimator in action, we will use the previous example of $f(x) = x^2$:\n",
    "\n",
    "$\\nabla_x f(x) = 2x$\n",
    "\n",
    "$\\nabla_{\\theta} g(\\theta) = 1$\n",
    "\n",
    "$\\nabla_x f(g(\\theta; \\epsilon)) \\nabla_{\\theta} g(\\theta) = 2 (\\theta + \\sigma \\epsilon)$\n",
    "\n",
    "Since $\\epsilon$ is $\\mathcal{N}(0, I)$, it has expectation zero, meaning that $\\mathbf{E}_{\\epsilon \\sim p}\\left[\\nabla_x f(x) \\nabla_{\\theta} g(\\theta)\\right] = 2\\theta$ at our parameter point $\\theta$.\n",
    "\n",
    "In code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(x) at point 3.00: 5.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grad_f = lambda x: 2 * x\n",
    "grad_x = lambda theta: 1\n",
    "\n",
    "samples = 1000\n",
    "theta = 3\n",
    "x = theta + np.random.normal(0, 1, samples)\n",
    "grad = np.mean(grad_f(x) * grad_x(theta))\n",
    "print(\"gradient of f(x) at point {:.2f}: {:.2f}\".format(theta, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine importance sampling with the change of variables estimator. If we analyze the simple example above, we can see that for a distribution $p(\\epsilon)$:\n",
    "\n",
    "$\\mu_\\epsilon = \\theta$\n",
    "\n",
    "$\\sigma_\\epsilon = \\sigma$\n",
    "\n",
    "Where we typically choose $\\sigma = 1$. For $f(x) = x^2$, our gradient function evaluates to:\n",
    "\n",
    "$2 (\\theta + \\sigma \\epsilon) = 2\\theta + \\sigma\\epsilon$\n",
    "\n",
    "As mentioned previously, since $\\mathbb{E}[\\epsilon] = 0$, the second term disappears, leaving us with the analytical gradient of $f(x)$. If $\\epsilon \\sim \\mathcal{N}(\\mu_q, \\sigma_q)$ for $\\mu_q \\neq 0$, the second term no longer disappears. To get the importance sampling estimate, we need to sample $\\epsilon \\sim q(\\epsilon)$ as above, and then do an importance sampling correction when taking the expectation:\n",
    "\n",
    "$x = \\theta + \\sigma_p \\epsilon$\n",
    "\n",
    "$\\mathbf{E}_{\\epsilon \\sim p}\\left[\\nabla_x f(x(\\theta)) \\nabla_{\\theta} g(\\theta)\\right] = \\mathbf{E}_{\\epsilon \\sim q}\\left[\\frac{p(\\epsilon)}{q(\\epsilon)}\\nabla_x f(x(\\theta)) \\nabla_{\\theta} x(\\theta)\\right]$\n",
    "\n",
    "For $\\epsilon \\sim q(\\epsilon)$.\n",
    "\n",
    "This is shown in the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(x) at point 3.00: 5.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi, log, sqrt\n",
    "\n",
    "grad_f = lambda x: 2 * x\n",
    "grad_x = lambda theta: 1\n",
    "normal_pdf = lambda x, mu, sigma: 1/(sigma * sqrt(2 * pi)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "samples = 100000\n",
    "theta = 3\n",
    "\n",
    "mu_p = theta\n",
    "sigma_p = 1\n",
    "\n",
    "mu_q = theta\n",
    "sigma_q = 15\n",
    "\n",
    "x = mu_p + sigma_p * np.random.normal(mu_q, sigma_q, size=samples)\n",
    "w = normal_pdf(x, mu_p, sigma_p) / normal_pdf(x, mu_q, sigma_q)\n",
    "grad = np.mean(w * grad_f(x) * grad_x(theta))\n",
    "print(\"gradient of f(x) at point {:.2f}: {:.2f}\".format(theta, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the score function estimator (empirically) has lower variance than the score function estimator, we run Monte Carlo simulations to find the gradient of the same function below, and compare the estimated variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score function gradient estimate:  5.98636276835\n",
      "reparameterized gradient estimate:  5.99773034292\n",
      "variance of score function method:  0.228829813057\n",
      "variance of reparameterization method:  0.00437956381245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grad_f = lambda x: 2 * x\n",
    "grad_x = lambda theta: 1\n",
    "grad_log_prob = lambda theta, x: (x - theta)\n",
    "\n",
    "theta = 3\n",
    "score_fn = []\n",
    "reparam = []\n",
    "for i in range(100):\n",
    "    samples = 1000\n",
    "    x = theta + np.random.normal(0, 1, samples)\n",
    "    glp = grad_log_prob(theta, x)\n",
    "    score_fn_grad = np.mean(glp * f(x))\n",
    "    reparam_grad = np.mean(grad_f(x) * grad_x(theta))\n",
    "    score_fn.append(score_fn_grad)\n",
    "    reparam.append(reparam_grad)\n",
    "    \n",
    "sf = np.array(score_fn)\n",
    "rp = np.array(reparam)\n",
    "print(\"score function gradient estimate: \", np.mean(sf))\n",
    "print(\"reparameterized gradient estimate: \", np.mean(rp))\n",
    "print(\"variance of score function method: \", np.std(sf) ** 2)\n",
    "print(\"variance of reparameterization method: \", np.std(rp) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see for this simple example, the change of variables method has on the order of 100x less variance than the score function method. The disadvantage of the method is that it requires $f(x)$ to be a differentiable function, whereas the score function method doesn't. This makes the score function method much more general, but it comes at the cost of high variance, which, for problems with a high number of dimensions, can make it inefficient or otherwise useless. To get around this, we will go through some general techniques for variance reduction that can be applied to Monte Carlo methods, and show how these can be used to reduce the variance of the score function estimator.\n",
    "\n",
    "## 4. Variance Reduction Techniques\n",
    "In this section, we'll cover a few variance reduction techniques that can improve our estimate of the value that we're interested in. The motivation here is that it lets us scale up our methods when we have many parameters, and makes solving more difficult problems feasible. In the case of the score function gradient estimator in particular, the variance of the estimator makes it ineffective in many applications without some form of variance reduction.\n",
    "\n",
    "Since the error of Monte Carlo estimates scales with $\\frac{\\sigma}{\\sqrt{n}}$, where $\\sigma$ is the standard deviation of the estimator, and $n$ is the number of samples, the two methods of reducing variance are either increasing the number of samples, or finding a better estimator. The below methods do exactly that. \n",
    "\n",
    "\n",
    "### 4.1 Control Variates\n",
    "A control variate typically involves using partial information to help make solving the problem easier. Consider:\n",
    "\n",
    "$\\mathbf{E}_{x \\sim p}\\left[f(x)\\right] = \\mathbf{E}_{x \\sim p}\\left[f(x) - b g(x)\\right] + b\\mathbf{E}_{x \\sim p}\\left[g(x)\\right]$\n",
    "\n",
    "Or alternatively:\n",
    "\n",
    "$\\mathbf{E}_{x \\sim p}\\left[f(x)\\right] = \\mathbf{E}_{x \\sim p}\\left[f(x) + b g(x)\\right] - b\\mathbf{E}_{x \\sim p}\\left[g(x)\\right]$\n",
    "\n",
    "\n",
    "This is just a fancy way of adding and subtracting the value of some function $g(x)$ that has zero net effect in expectation. The variance of this new estimator is:\n",
    "\n",
    "$Var(f - g) = Var(f) - 2bCov(f, g) + b^2Var(g)$\n",
    "\n",
    "$Var(f + g) = Var(f) + 2bCov(f, g) + b^2Var(g)$\n",
    "\n",
    "If the two functions $f$ and $g$ are correlated, then $Var(f - g)$ of a Monte Carlo estimate of f can be lower than $Var(f)$ (and similarly if the two functions are inversely correlated). Furthermore, the form of the variance suggests that there is an optimal constant $b$ that reduces variance to a minimum. Consider:\n",
    "\n",
    "$\\frac{d}{db}Var(f + g) = 2Cov(f, g) + 2bVar(g) = 0$\n",
    "\n",
    "$b^* = -\\frac{cov(f, g)}{Var(g)}$\n",
    "\n",
    "This implies that any correlated function will lower the variance of our Monte Carlo estimator, provided we can compute $b^*$, as the sign of the correlation will be absorbed into $b^*$.\n",
    "\n",
    "To illustrate control variates in action, we will integrate the funtion:\n",
    "\n",
    "$f(u) = \\int_{0}^{1} \\frac{1}{u + 1} du$\n",
    "\n",
    "Choosing the control variate:\n",
    "\n",
    "$g(u) = \\int_{0}^{1} (u + 1) du = \\left[\\frac{u^2}{2} + u\\right]_{0}^1 = \\frac{3}{2}$\n",
    "\n",
    "Our problem now becomes:\n",
    "\n",
    "$\\mathbf{E}_{x \\sim p} \\left[\\int_0^1 f(u) du\\right] = \\mathbf{E}_{x \\sim p} \\left[\\int_0^1 \\frac{1}{u + 1} du + \\int_{0}^{1} (u + 1) du\\right] - 3/2$\n",
    "\n",
    "Solving this problem below, with the constant $b \\approx 0.4773$ gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv integral estimate:  0.693157238984\n",
      "std integral estimate:  0.693676776286\n",
      "cv2 integral estimate:  0.694196313587\n",
      "variance of control variate method:  3.57202528261e-07\n",
      "variance of standard method:  1.2601542731e-05\n",
      "variance of control variate 2:  4.92982212049e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi, log\n",
    "\n",
    "f = lambda u: 1 / (u + 1)\n",
    "g = lambda u: u + 1\n",
    "\n",
    "samples = 1500\n",
    "a, b = 0, 1\n",
    "c = 0.4773\n",
    "\n",
    "control_variate = []\n",
    "standard = []\n",
    "cv2 = []\n",
    "for i in range(100):\n",
    "    u = np.random.uniform(a, b, samples)\n",
    "    int_f = np.mean(f(u))*(b - a)\n",
    "    int_g = np.mean(g(u))*(b - a)\n",
    "    total = int_f + c*(int_g - 3/2)\n",
    "    control_variate.append(total)\n",
    "    standard.append(int_f)\n",
    "    cv2.append(int_f - c*(int_g - 3/2))\n",
    "    \n",
    "cv = np.array(control_variate)\n",
    "cv2 = np.array(cv2)\n",
    "st = np.array(standard)\n",
    "print(\"cv integral estimate: \", np.mean(cv))\n",
    "print(\"std integral estimate: \", np.mean(st))\n",
    "print(\"cv2 integral estimate: \", np.mean(cv2))\n",
    "print(\"variance of control variate method: \", np.std(cv) ** 2)\n",
    "print(\"variance of standard method: \", np.std(st) ** 2)\n",
    "print(\"variance of control variate 2: \", np.std(cv2) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the control variate method has on the order of 100 times less variance than the standard Monte Carlo estimate. Typically, a control variate will not have such a profound effect on the variance, though reductions of 25-30% are reasonably common. Notably, the second method where we reversed the correlation between the control variate and $f(u)$ resulted in higher overall variance than the standard estimator (note that this is the minimum variance). This is something to be aware of, and why it is critical to calculate the optimatal constant $b^*$ (this may involve solving a least-squares problem using an estimate of the variance and covariance).\n",
    "\n",
    "Finally, we can also stack control variates to reduce variance even further -- that is, we can use as many control variates as we like, so long as the additional functions are correlated with the function that we're interested in, and easy to solve. For each control variate, we want to solve for the constant that minimizes variance, which results in a least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Common Random Numbers\n",
    "Common random numbers are typically used when you want to estimate a value of the form:\n",
    "\n",
    "$\\mathbb{E}\\left[f(x)\\right] - \\mathbb{E}\\left[g(x)\\right] = \\mu_1 - \\mu_2$\n",
    "\n",
    "As will become a common theme in our variance reduction methods, if we can find some way of introducing correlation into our function approximation, we can use this to reduce the variance of our estimate. In the case of control variates, we did this by introducing a new function that is correlated with our original function, and finding a tuning constant that provably reduces variance. In the case of common random numbers, rather than sampling $x_1 \\sim p$ and $x_2 \\sim q$, and estimating $\\mathbb{E}\\left[f(x_1)\\right] - \\mathbb{E}\\left[g(x_2)\\right]$ we will instead come up with some procedure $x_3$ such that our samples of $x_1$ and $x_2$ are correlated, and estimate our function using these correlated samples.\n",
    "\n",
    "So how do we do this? One way that is covered further on is called the antithetic variate, which generates negatively correlated samples. In this case, however, we will use a method called a copula to sample $x$ and $y$ from a correlated multivariate distribution. The idea behind this method is that we can convert between distributions by using a uniform distribution as a lingua franca. For example, if we sample $x \\sim \\mathcal{N}(0, I)$, $CDF(x)$ will be uniformly distributed. We can use the inverse CDF of a known function to transform our uniform distribution into a different distribution.\n",
    "\n",
    "So how do we use this? Well, we first define a bivariate normal distribution, and sample $x, y$. Next, we use the CDF for the normal distribution to transform our samples into correlated uniform samples, and then evaluate our function. This seems like a lot, but in code it's fairly straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv integral estimate:  -0.806923207618\n",
      "std integral estimate:  -0.808428911966\n",
      "variance of control variate method:  3.18745160412e-05\n",
      "variance of standard method:  0.000104896021892\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEWCAYAAADoyannAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi, log\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda u: 1 / (u + 1)\n",
    "g = lambda u: u + 1\n",
    "\n",
    "samples = 1500\n",
    "a, b = 0, 1\n",
    "c = 0.4773\n",
    "corr = -np.linspace(0, 0.99, 100)\n",
    "\n",
    "mu = np.zeros((2,))\n",
    "norm = stats.norm()\n",
    "data_crn = []\n",
    "data_std = []\n",
    "for c in corr:\n",
    "    crn = []\n",
    "    standard = []\n",
    "    cov = np.array([[1, c],\n",
    "                    [c, 1]])\n",
    "    mvnorm = stats.multivariate_normal(mean=mu, cov=cov)\n",
    "    for i in range(100):\n",
    "        u = np.random.uniform(a, b, samples)\n",
    "        x = mvnorm.rvs(int(samples/2))\n",
    "        u_1, u_2 = norm.cdf(np.array(x[:,0])), norm.cdf(np.array(x[:,1]))\n",
    "        f_x = np.mean(f(u))\n",
    "        g_x = np.mean(g(u))\n",
    "        crn_est = np.mean(f(u_1) - g(u_2))\n",
    "        std = f_x - g_x\n",
    "        crn.append(crn_est)\n",
    "        standard.append(std)\n",
    "    \n",
    "    crn = np.array(crn)\n",
    "    standard = np.array(standard)\n",
    "    data_crn.append(np.std(crn) ** 2)\n",
    "    data_std.append(np.std(standard) ** 2)\n",
    "print(\"cv integral estimate: \", np.mean(crn))\n",
    "print(\"std integral estimate: \", np.mean(standard))\n",
    "print(\"variance of control variate method: \", np.std(crn) ** 2)\n",
    "print(\"variance of standard method: \", np.std(standard) ** 2)\n",
    "\n",
    "# imperative style plotting in pyplot\n",
    "plt.plot(-corr, np.array(data_crn))\n",
    "plt.plot(-corr, np.array(data_std))\n",
    "plt.xlabel('Correlation in x and y')\n",
    "plt.ylabel('Variance (100 Trials)')\n",
    "plt.title('Variance of Correlated vs Uncorrelated Monte Carlo Estimation')\n",
    "plt.legend([\"CRN\", \"STD\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this example, we managed a roughly 10-fold reduction in variance using correlated sampling. Not bad for a first cut! Plotting the variance as a function of the negative correlation in our copula, we can see that increasing the correlation decreases variance almost linearly in the regime we're looking at. As expected, with zero correlation, there's no difference between the estimates of the two methods.\n",
    "\n",
    "### 4.3 Conditioning\n",
    "\n",
    "Conditioning (sometimes called Rao-Blackwellization) is where we partially solve the problem analytically, to reduce its dimensionality. Rather than solving the full multidimensional problem, we reduce the dimensionality and solve a lower dimensional problem instead. This is beneficial because conditional Monte Carlo estimation cannot have higher variance than ordinary Monte Carlo estimation, and will typically have strictly lower variance. This is because:\n",
    "\n",
    "$Var(f(x,y)) = \\mathbf{E}\\left[Var(f(x,y)|x)\\right] + Var(\\mathbf{E}\\left[f(x,y)|x\\right])$\n",
    "\n",
    "By partially solving the problem, we eliminate the variance associated with $h(x) = \\mathbf{E}\\left[f(x,y)|x\\right]$, and so the variance of a Monte Carlo solution to $h(x)$ will always be strictly less than or equal to the variance of a Monte Carlo solution of $f(x,y)$.\n",
    "\n",
    "For a concrete example, consider the function:\n",
    "\n",
    "$f(x, y) = \\int_{0}^2\\int_{0}^1 e^{g(x)y} dx dy$\n",
    "\n",
    "For $g(x) = \\sqrt{5/4 + \\cos(2\\pi x)}$\n",
    "\n",
    "We can hold $g(x)$ constant and integrate out $y$, leaving $h(x) = (e^{2 g(x)} - 1)/g(x)$. Solving this problem will yield the same result as the original, with much lower variance, as we will see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioned integral estimate:  7.79021514443\n",
      "std integral estimate:  7.76403442803\n",
      "variance of conditioned method:  0.00604412984845\n",
      "variance of standard method:  0.0248584548282\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "f = lambda x, y : np.exp(g(x) * y)\n",
    "g = lambda x : np.sqrt(5/4 + np.cos(2 * pi * x))\n",
    "h = lambda x : (np.exp(2 * g(x)) - 1)/g(x)\n",
    "\n",
    "samples = 1500\n",
    "a, b = 0, 1\n",
    "c, d = 0, 2\n",
    "\n",
    "conditioned = []\n",
    "standard = []\n",
    "for i in range(100):\n",
    "    x = np.random.uniform(a, b, int(samples))\n",
    "    y = np.random.uniform(c, d, int(samples))\n",
    "    z = np.random.uniform(a, b, samples)\n",
    "    \n",
    "    f_xy = np.mean(f(x, y)) * (b - a) * (d - c) \n",
    "    h_x = np.mean(h(z)) * (b - a)\n",
    "    \n",
    "    standard.append(f_xy)\n",
    "    conditioned.append(h_x)\n",
    "    \n",
    "cnd = np.array(conditioned)\n",
    "st = np.array(standard)\n",
    "print(\"conditioned integral estimate: \", np.mean(cnd))\n",
    "print(\"std integral estimate: \", np.mean(st))\n",
    "print(\"variance of conditioned method: \", np.std(cnd) ** 2)\n",
    "print(\"variance of standard method: \", np.std(st) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the conditioned estimator has much lower variance than our estimate of the original function. Though this technique will always yield an estimator that is as good as or better than the original, it may nevertheless not be worth implementing in cases where partially solving the function results in a more expensive function evaluation than the original. For example, the function:\n",
    "\n",
    "$f(x) = \\cos\\left(g(x_1) + \\sum_{i=1}^d a_i x_i\\right)$\n",
    "\n",
    "Can be conditioned to give:\n",
    "\n",
    "$h(x) = \\left(\\sin\\left(g(x_1) + \\sum_{i=1}^{d-1} a_i x_i + a_d\\right)\\right) - \\left(\\sin\\left(g(x_1) + \\sum_{i=1}^{d-1} a_i x_i\\right)\\right)$\n",
    "\n",
    "Doing so results in an estimator that requires $2\\times (d-1)$ evaluations of the trig functions, instead of just $d$, meaning that our estimator will now be much slower. This type of scenario is something to keep in mind when applying this technique.\n",
    "\n",
    "### 4.4 Antithetic Variates\n",
    "The idea of antithetic variates is that we can generate a new sample $\\mathbf{X}$ based on our knowledge of $f(X)$ such that $f(\\mathbf{X})$ is negatively correlated with $f(X)$. When this is the case, we can reduce the variance of our estimate of $f(X)$. Denoting $f(\\mathbf{X})$ as $g(X)$:\n",
    "\n",
    "$Var\\left(\\frac{f + g}{2}\\right) = \\frac{Var(f) + 2 Cov(f,g) + Var(g)}{4}$\n",
    "\n",
    "The idea of this method is that we generate samples $u_0, u_1, u_2, ..., u_N$ and a second set of samples $v_0, v_1, v_2, ..., v_N$ that are negatively correlated with $u$. For $u \\sim \\mathcal{U}(a,b)$, we can do this using $v = b - u + a$. For $u \\sim \\mathcal{N}(\\mu, \\sigma)$, then $v = -u$. From there, we can estimate the function using:\n",
    "\n",
    "$\\mathbb{E}\\left[f(x)\\right] = \\frac{\\mathbb{E}\\left[f(u)\\right] + \\mathbb{E}\\left[f(v)\\right]}{2}$\n",
    "\n",
    "This estimator is unbiased, and will have lower variance than the standard Monte Carlo estimation. In code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av integral estimate:  4.32268697567\n",
      "std integral estimate:  4.32468800581\n",
      "variance of antithetic variate method:  0.00505727763704\n",
      "variance of standard method:  0.0242148377355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = lambda x : x**2\n",
    "\n",
    "samples = 1000\n",
    "a, b = -1, 4\n",
    "\n",
    "antithetic_variate = []\n",
    "standard = []\n",
    "for i in range(100):\n",
    "    u = np.random.uniform(a, b, samples)\n",
    "    v = b - u[:int(samples/2)] + a\n",
    "    f_x = np.mean(f(u))\n",
    "    f_anti = np.mean((f(u[:int(samples/2)])+f(v))/2)\n",
    "    antithetic_variate.append(f_anti)\n",
    "    standard.append(f_x)\n",
    "    \n",
    "av = np.array(antithetic_variate)\n",
    "st = np.array(standard)\n",
    "print(\"av integral estimate: \", np.mean(av))\n",
    "print(\"std integral estimate: \", np.mean(st))\n",
    "print(\"variance of antithetic variate method: \", np.std(av) ** 2)\n",
    "print(\"variance of standard method: \", np.std(st) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above example, the antithetic variate estimate has on the order of 10-100 times less variance than the standard method, even for the same number of samples.\n",
    "\n",
    "### 4.5 Stratification\n",
    "Stratification involves breaking the sampling space up into smaller intervals, and sampling from each of those intervals separately. Intuitively, this gives us a more even coverage of the space, and so reduces the variance of the estimate (taking this to the extreme, numerical integration with a fixed step size can be seen as a special case of stratification, and has a variance of 0).\n",
    "\n",
    "One of the inherent advantages of stratification is that we can save computation in regions that are relatively flat, by drawing fewer samples there. Intuitively, if $f(x)$ is constant in the interval $[a,b]$ we can know the value of $f(x)$ in that region with only a single function call. We can't assume to know this in a typical case, though with some analysis of our function, it may become reasonably apparent.\n",
    "\n",
    "An example of stratification is given in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard estimate is:  5.338110278\n",
      "Stratified estimate is:  5.33485110099\n",
      "Standard std is:  0.15527153461\n",
      "Stratified std is:  0.0152856249486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = lambda x : x ** 2\n",
    "\n",
    "samples = 1000\n",
    "a, b = 0, 4\n",
    "strata = 10\n",
    "da = (b - a) / strata\n",
    "s = int(samples / strata)\n",
    "standard = []\n",
    "stratified = []\n",
    "for i in range(100):\n",
    "    a_ = 0\n",
    "    f_x = 0\n",
    "    for j in range(strata):\n",
    "        u = np.random.uniform(a_, a_ + da, s)\n",
    "        f_x += np.sum(f(u))\n",
    "        a_ += da\n",
    "    f_x /= samples\n",
    "    v = np.random.uniform(a, b, samples)\n",
    "    f_v = np.mean(f(v))\n",
    "    standard.append(f_v)\n",
    "    stratified.append(f_x)\n",
    "\n",
    "standard = np.array(standard)\n",
    "stratified = np.array(stratified)\n",
    "print(\"Standard estimate is: \", np.mean(standard))\n",
    "print(\"Stratified estimate is: \", np.mean(stratified))\n",
    "print(\"Standard std is: \", np.std(standard))\n",
    "print(\"Stratified std is: \", np.std(stratified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in this case, the variance of the estimate is reduced to a 10th of the normal Monte Carlo estimate.\n",
    "\n",
    "## 5. Conclusion\n",
    "In this workbook, we went through a few common Monte Carlo function approximation methods that see common use in areas such as finance, and machine learning / AI. In addition, we covered a several variance reduction techniques that can dramatically improve the performance of the given methods, and make difficult problems more tractable. Though the examples given are necessarily straightforward, it is nevertheless hoped that they provide a springboard for more interesting applications of these techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "Machine learning is any process where we train some function to learn a representation of a set of underlying data. At its simplest, you can think of this as fitting a line to a set of scatter points in Excel, and at its most complex, you can think of it as learning a mapping from sensory input (cameras, LIDAR, sonar, IR, IMU, etc.) to a set of actuations. \n",
    "\n",
    "In general, we need four components:\n",
    "\n",
    "* A **dataset** to train on;\n",
    "* A **function or architecture** that we wish to train, parameterized by a set of weighting parameters $\\Theta$;\n",
    "* A **cost or value function** $f: X \\rightarrow \\mathbb{R}$ that tells us how \"good\" our function fit is; and,\n",
    "* An **optimization method**.\n",
    "\n",
    "In general, we can think of machine learning as having the following sub-disciplines:\n",
    "\n",
    "* **Regression**, in which we fit a function to a set of known datapoints;\n",
    "* **Classification**, in which we decide if an object is a member of a discrete set of classes; and,\n",
    "* **Clustering**, in which objects are ordered in a space according to the relative similarity of their properties.\n",
    "\n",
    "Each of these disciplines is useful in their own way -- for example, clustering can be used to learn how objects relate to one another (which has uses as diverse as image compression), regression can be used to learn functions that map an input to a useful output (e.g. a controller that maps a state input to an action output), and classification can be used for categorical decision-making (i.e. \"if there is a dog in the picture, bark\"). In principle, we can build machines that combine each of these components together to leverage the advantages of each.\n",
    "\n",
    "We can further categorize these tasks as being either **supervised**, or **unsupervised**. In supervised learning tasks, we have a set of inputs (known as features) and a set of outputs (known as labels) that we wish to replicate. We train the output of our function to match the labels that are given to us. Regression and classification are typical supervised learning tasks. In contrast, in unsupervised learning, we have no labels, and instead learn how objects relate to one another. Clustering is typical of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. This Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop will focus on teaching students the basics of machine learning, from polynomial and logistic regression, through to using neural networks for advanced tasks such as automated clustering, and regression of very complex functions. This is by no means an exhaustive treatment of the subject, and it should really be treated as a necessary introduction to more advanced topics such as Generative Models (VAEs, GANs, VAEGANs), and Deep Reinforcement Learning.\n",
    "\n",
    "I will be breaking this course down into the following sections:\n",
    "\n",
    "#### 2.1 Regression\n",
    "* Fitting basic functions to data using inbuilt numpy / scipy tools;\n",
    "* Introductory information theory; i.e. what the hell just happened?\n",
    "* The basics of optimization;\n",
    "* Implementations of two relevant modern techniques -- gradient descent and evolutionary algorithms  -- which can be used as a stepping stone to understanding more advanced techniques (i.e. second order gardient methods, CMA-ES);\n",
    "* The backpropagation algorithm -- the workhorse of deep learning, in which neural nets are trained using gradient descent in combination with the chain rule; and,\n",
    "* Neural network function regression using PyTorch. We will go through implementations of both a simple function regression exercise, and a time-series regression exercise using recurrent neural nets (RNNs).\n",
    "\n",
    "#### 2.2 Classification\n",
    "* We will expand on some of the ideas learned in the regression module to include the idea of choosing between discrete classes;\n",
    "* We will introduce two separate ways of treating classification tasks, and train both methods using scipy optimizers. This will give you a basic understanding of classical methods before the next section.\n",
    "* We will then look at neural network classification, and go through a simple MNIST hand-written digit classifier. The aim of this exercise is to expand on previous exercises to show that going from classical methods to neural networks is a small jump.\n",
    "* Finally, We will expand on the principles we've learned to go through a simple convolutional neural network (CNN) -- generally the gold standard for working with images.\n",
    "\n",
    "#### 2.3 Clustering\n",
    "* We will expand our examples to include unsupervised learning (clustering).\n",
    "* We introduce a traditional method of clustering known as K-Means. We will go through a simple example, and use it to cluster a synthetic dataset.\n",
    "* We will introduce Gaussian Mixture Models -- a soft version of the K-Means algorithm -- and go through the expectation maximization algorithm. We will use it to cluster a 1D dataset.\n",
    "* We will learn how neural networks can be used to perform automatic clustering, including the loss function that we need to use.\n",
    "* We go through a basic implementation of an autoencoder so that students can get an idea of how the clustering happens.\n",
    "\n",
    "Through the course of these notebooks, you will see both the fundamental theory underlying the methods we cover, as well as working implementations that translate the mathematical intuition into working code. If you fully understand the material covered, you will be in good stead to apply these techniques to more complex problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

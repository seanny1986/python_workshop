{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a separate discipline to the previous branches of machine learning that we've looked at. RL is a colossal field that is difficult to cover in a single notebook, but I'l try to cover some of the functional similarities and differences it has with other branches of machine learning.\n",
    "\n",
    "RL is generally a control problem: it is focused on the behaviour of an agent in an environment. The agent has multiple sensors -- which may be noisy -- through which it perceives its environment. At every time step, it takes an action based on its observation, and receives a reward from the environment. This sequence repeats, with the agent receiving rewards commensurate with its actions. This is summarized in the following diagram:\n",
    "\n",
    "The goal of the agent is to maximize the overall reward it receives, by improving its actions. That is, it should learn from its previous actions and observed rewards in order to take better actions in future, thus receiving better rewards. RL is central to the current advances that have happened in AI, with many of the major headline-grabbing feats being a result of this simple formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decisions Processes (MDPs) are central to RL. In MDPs, we assume that all the information necessary to make a decision about the future is given in the present observation space. That is, the past and the future are independent of another given the present. To demonstrate the process, we can start with an example where this is obviously false: a robot with a battery that navigates from A to B. Depending on the path the robot took, it could wind up in an arbitrary state $s_{t}$ with different levels of charge. As such, it may not have enough energy to navigate to point B. We can trivially solve this problem by including the battery charge in the agent's observation space.\n",
    "\n",
    "This is the second assumption underlying an MDP: that we can include all information necessary into the observation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have two (free-to-play) slot machines, and each one has a different probability of paying out when you pull on its lever. Your goal in this game is to make as much money as possible, which can be understood as finding the machine with the best payout, and then only using that machine. Initially, you know nothing about either machine, so you have to test both machines out to get an idea of which one provides the best return. Once you have a pretty good idea of which machine that is, you can exploit the knowledge you've earned to maximize your total reward.\n",
    "\n",
    "This example illustrates the crux of the exploration-exploitation trade-off. You want to maximize your reward, but in the early stages of learning, you must explore different machines to determine which one is optimal. In doing so, you decrease the maximum reward that you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Policy** is a means of selecting an action. For example, given a choice between going left and right, we might be 80% certain that going left is the best choice, and 20% certain that going right is the best choice. A valid policy might be to go left 80% of the time, and go right 20% of the time. Similarly, we could go left with some high probability $P(x)$ and choose a random action with some probability $1-P(x)$.\n",
    "\n",
    "The purpose of a **value function** is to tell us how good or bad an input is. That is, it tells us the discounted sum of rewards, or return, that we can expect on average, under the current policy. There are different types of return: the state value function, and the state-action value function:\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s_{t}) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=t}^{T}\\gamma^{k-t}r_{k}|s_{t}=s\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Q^{\\pi}(s_{t}, a_{t}) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=t}^{T}\\gamma^{k-t}r_{k}|s_{t}=s, a_{t}=a\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The state value function is the expected value of the current state under the policy, and can be imagined as rolling out (infinitely) many trajectories from the current state, and then calculating the average return. The State-action value function can be thought of as the value of taking an action from the current state, and *then* rolling out infinitely many trajectories from the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value-based methods work in discrete action spaces, where we can make a choice about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "\n",
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.l1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.val = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.l1(x))\n",
    "        return F.softmax(self.val(x), dim=-1)\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, eval_net, target_net):\n",
    "        super(DQN, self).__init__()\n",
    "        self.eval_net = eval_net\n",
    "        self.target_net = target_net\n",
    "        self.hard_update(self.target_net, self.eval_net)\n",
    "        \n",
    "        self.tau = 0.01\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.memory_len = 50000\n",
    "        self.memory = []\n",
    "    \n",
    "    def soft_update(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data*(1.0-tau)+param.data*tau)\n",
    "\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "    def select_action(self, x):\n",
    "        scores = self.eval_net(x)\n",
    "        dist = Categorical(scores)\n",
    "        action = dist.sample()\n",
    "        return action\n",
    "    \n",
    "    def add_to_memory(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.memory_len:\n",
    "            self.memory.pop()\n",
    "    \n",
    "    def sample_memory(self, batch_size=64): \n",
    "        if len(self.memory) < batch_size:\n",
    "            return self.memory\n",
    "        else:\n",
    "            samples = [self.memory[random.randint(0,len(self.memory)-1)] for i in range(batch_size)]\n",
    "            return samples\n",
    "    \n",
    "    def train(self, optimizer):\n",
    "        batch = self.sample_memory()\n",
    "        \n",
    "        states = torch.stack([t[\"states\"] for t in self.memory])\n",
    "        actions = torch.stack([t[\"actions\"] for t in self.memory]).unsqueeze(dim=1)\n",
    "        next_states = torch.stack([t[\"next_states\"] for t in self.memory])\n",
    "        rewards = torch.stack([t[\"rewards\"] for t in self.memory]).squeeze(dim=1)\n",
    "        \n",
    "        q_eval = torch.gather(self.eval_net(states), 1, actions).squeeze(dim=1)\n",
    "        q_next = self.target_net(states).detach()\n",
    "        q_target = rewards+self.gamma*q_next.max(dim=1)[0]\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.target_net, self.eval_net, self.tau)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our agent coded up, let's write the training loop. Essentially, at every time-step, the agent will sample an action, take it, make an observation, and then push this observation through to its memory. Then, the agent will sample a random batch from memory, and perform a Q-learning update. Over time, the agent will get progressively better and better at performing the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-14 17:39:49,527] Making new env: CartPole-v0\n",
      "/Users/seanmorrison/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  22.0\n",
      "Reward:  10.0\n",
      "Reward:  24.0\n",
      "Reward:  11.0\n",
      "Reward:  9.0\n",
      "Reward:  8.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  8.0\n",
      "Reward:  10.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  8.0\n",
      "Reward:  10.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  10.0\n",
      "Reward:  8.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  11.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  8.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e7d8c2b7043e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-9fb550ef616c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"states\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"next_states\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "eval_net = QNet(state_dim, 32, action_dim)\n",
    "target_net = QNet(state_dim, 32, action_dim)\n",
    "agent = DQN(eval_net, target_net)\n",
    "optim = torch.optim.Adam(eval_net.parameters(), lr=1e-3)\n",
    "\n",
    "iterations = 500\n",
    "warmup = 20\n",
    "log_interval = 10\n",
    "running = True\n",
    "for i in range(iterations):\n",
    "    total_reward = 0\n",
    "    state = torch.Tensor(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.numpy())\n",
    "        total_reward += reward\n",
    "        transition = {\"states\": state,\n",
    "                     \"actions\": action,\n",
    "                     \"rewards\": torch.Tensor([reward]),\n",
    "                     \"next_states\": torch.Tensor(next_state)}\n",
    "        agent.add_to_memory(transition)\n",
    "        if i > warmup:\n",
    "            agent.train(optim)\n",
    "        state = torch.Tensor(next_state)\n",
    "    if i % log_interval == 0:\n",
    "        print(\"Reward: \", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent gradually improves its performance until it has perfected the task. Below, we do a quick comparison using a probabilistic method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNProb(DQN):\n",
    "    def __init__(self, eval_net, target_net):\n",
    "        super(DQNProb, self).__init__(eval_net, target_net)\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        scores = self.eval_net(x)\n",
    "        dist = Categorical(scores)\n",
    "        action = dist.sample()\n",
    "        return action, scores\n",
    "    \n",
    "    def train(self, optimizer):\n",
    "        batch = self.sample_memory()\n",
    "        \n",
    "        states = torch.stack([t[\"states\"] for t in self.memory])\n",
    "        actions = torch.stack([t[\"actions\"] for t in self.memory]).unsqueeze(dim=1)\n",
    "        probs = torch.stack([t[\"probs\"] for t in self.memory])\n",
    "        next_states = torch.stack([t[\"next_states\"] for t in self.memory])\n",
    "        rewards = torch.stack([t[\"rewards\"] for t in self.memory]).squeeze(dim=1)\n",
    "        \n",
    "        q_eval = torch.gather(self.eval_net(states), 1, actions).squeeze(dim=1)\n",
    "        q_next = self.target_net(states)#.detach()\n",
    "        q_target = rewards+self.gamma*torch.sum(probs*q_next, dim=1)\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.target_net, self.eval_net, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is almost the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-15 12:05:07,389] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  25.0\n",
      "Reward:  19.0\n",
      "Reward:  12.0\n",
      "Reward:  11.0\n",
      "Reward:  11.0\n",
      "Reward:  13.0\n",
      "Reward:  9.0\n",
      "Reward:  12.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  8.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  10.0\n",
      "Reward:  9.0\n",
      "Reward:  9.0\n",
      "Reward:  13.0\n",
      "Reward:  11.0\n",
      "Reward:  10.0\n",
      "Reward:  10.0\n",
      "Reward:  8.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-c86906ebbb4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-ac665e6355ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"states\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"probs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "eval_net = QNet(state_dim, 32, action_dim)\n",
    "target_net = QNet(state_dim, 32, action_dim)\n",
    "agent = DQNProb(eval_net, target_net)\n",
    "optim = torch.optim.Adam(eval_net.parameters(), lr=1e-4)\n",
    "\n",
    "iterations = 500\n",
    "warmup = 20\n",
    "log_interval = 10\n",
    "running = True\n",
    "for i in range(iterations):\n",
    "    total_reward = 0\n",
    "    state = torch.Tensor(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.numpy())\n",
    "        total_reward += reward\n",
    "        transition = {\"states\": state,\n",
    "                     \"actions\": action,\n",
    "                     \"probs\": prob,\n",
    "                     \"rewards\": torch.Tensor([reward]),\n",
    "                     \"next_states\": torch.Tensor(next_state)}\n",
    "        agent.add_to_memory(transition)\n",
    "        if i > warmup:\n",
    "            agent.train(optim)\n",
    "        state = torch.Tensor(next_state)\n",
    "    if i % log_interval == 0:\n",
    "        print(\"Reward: \", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy-based methods typically fall into the space of policy iteration, and policy gradients. We will cover policy gradients primarily, as they are one of the current techniques underlying the success of modern breakthroughs like AlphaGo and OpenAI Five.\n",
    "\n",
    "Unlike value-based methods, which aim to learn a value function and apply an implicit policy, policy-based methods aim to learn a policy directly, by mapping states to actions using a function $\\pi : \\mathcal{S}\\rightarrow\\mathcal{A}$. $\\pi$ can be either deterministic or stochastic, though stochastic is more common. Typically we place a distribution over actions (call it $\\pi_{\\theta}(a|s)$), and sample from it. As the policy is trained, the mean shifts towards the best action for that state, and the variance tends to zero, making the policy more deterministic.\n",
    "\n",
    "Policy gradients use a technique called the score function estimator to be able to estimate the derivative for the policy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{x~p}\\left[f(x)\\right] = \\int_{x}p_{\\theta}(x)f(x)dx \n",
    "\\end{equation}\n",
    "\n",
    "Where $f(x)$ is some cost function, and we want to take the gradient of $f(x)$. We can differentiate under the integral sign as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{x~p}\\left[\\nabla_{\\theta}f(x)\\right] = \\int_{x}\\frac{p_{\\theta}(x)}{p_{\\theta}(x)}\\nabla_{\\theta}p_{\\theta}(x)f(x)dx \n",
    "\\end{equation}\n",
    "\n",
    "Which we can rearrange as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{x~p}\\left[\\nabla_{x}f(x)\\right] = \\int_{x}p_{\\theta}(x)\\frac{\\nabla_{\\theta}p_{\\theta}(x)}{p_{\\theta}(x)}f(x)dx \n",
    "\\end{equation}\n",
    "\n",
    "Next, we sub in the identity:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\nabla_{x}g(x)}{g(x)} = \\nabla_{x}\\log{g(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{x~p}\\left[\\nabla_{\\theta}f(x)\\right] = \\int_{x}p_{\\theta}(x)\\log{p_{\\theta}(x)}f(x)dx = \\mathbb{E}_{x~p}[\\nabla_{\\theta}\\log{p_{\\theta}(x)}f(x)]\n",
    "\\end{equation}\n",
    "\n",
    "This lets us estimate the gradient for any function, provided we can parameterize some distribution that we can sample from. To turn this into something meanigful that we can use, we assume that $f(x)$ is some return function (say, the Q function or the advantage function), and our probability distribution is parameterized by some parameters $\\theta$ that we want to learn. Unlike the above example, we have a joint probability distribution over states and actions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{s,a~\\pi}\\left[\\nabla_{\\theta}Q^{\\pi}(s_{t},a_{t})\\right] = \\int_{s}\\rho^{\\pi}(s)\\int_{a}\\pi_{\\theta}(a_{t}|s_{t})\\nabla_{\\theta}\\log{\\pi_{\\theta}}(a_{t}|s_{t})Q^{\\pi}(s_{t},a_{t})\\,da\\, ds \n",
    "\\end{equation}\n",
    "\n",
    "Where $\\rho^{\\pi}(s)$ is the discounted state visitation frequency under the policy $\\pi$. Simplifying, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{s,a~\\pi}\\left[\\nabla_{\\theta}Q^{\\pi}(s_{t},a_{t})\\right] = \\mathbb{E}_{\\pi}\\left[\\nabla_{\\theta}\\log{\\pi_{\\theta}}(a_{t}|s_{t})Q^{\\pi}(s_{t},a_{t}) \\right] \n",
    "\\end{equation}\n",
    "\n",
    "This gives us a useful expression for estimating the gradient of our agent with respect to the parameters $\\theta$, which we can then use to do gradient ascent on the reward (i.e. step our parameters in the direction that maximizes the reward). In practice, we don't need to use $Q^{\\pi}(s,a)$; we can use any appropriate return or cost function for the task at hand. Since the score function estimator is high variance, it's common practice to subtract a constant baseline (this is a special case of the method of control variates for variance reduction). We can do this because $\\nabla_{\\theta} b = 0$ for any constant $b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "\n",
    "class PolicyGradient(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.actor = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(input_dim, hidden_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.l1(x))\n",
    "        score = F.softmax(self.score(x), dim=-1)\n",
    "        value = self.value(x)\n",
    "        return score, value\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        score, value = self.forward(x)\n",
    "        dist = Categorical(score)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, value, log_prob\n",
    "        \n",
    "    def train(self, trajectory, optimizer):\n",
    "        rewards = torch.stack(trajectory[\"rewards\"])\n",
    "        log_probs = torch.stack(trajectory[\"log_probs\"]).unsqueeze(dim=1)\n",
    "        values = torch.stack(trajectory[\"values\"])\n",
    "        masks = torch.stack(trajectory[\"dones\"])\n",
    "        \n",
    "        returns = torch.Tensor(rewards.size(0),1)\n",
    "        deltas = torch.Tensor(rewards.size(0),1)\n",
    "        \n",
    "        prev_return = 0\n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            returns[i] = rewards[i]+self.gamma*prev_return*masks[i]\n",
    "            prev_return = returns[i, 0]\n",
    "        deltas = returns-values\n",
    "        returns = (returns-returns.mean())/returns.std()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        actor_loss = -(log_probs*deltas.detach()).mean()\n",
    "        critic_loss = torch.mean(deltas**2)\n",
    "        loss = actor_loss+critic_loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop is conceptually similar to the one used for value-based methods, except we roll-out full trajectories and then train offline, rather than training online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = PolicyGradient(state_dim, 32, action_dim)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-2)\n",
    "\n",
    "iterations = 500\n",
    "batch_size = 1024\n",
    "epochs = 4\n",
    "log_interval = 10\n",
    "\n",
    "avg = 0\n",
    "int_avg = 0\n",
    "int_counter = 1\n",
    "for ep in range(1, iterations+1):\n",
    "    r_, v_, lp_, dones = [], [], [], []\n",
    "    t = 0\n",
    "    eps = 0\n",
    "    batch_mean = 0\n",
    "    while t < batch_size:\n",
    "        state = torch.Tensor(env.reset())\n",
    "        running_reward = 0\n",
    "        done = False\n",
    "        while not done:          \n",
    "            action, value, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "            running_reward += reward\n",
    "            next_state = torch.Tensor(next_state)\n",
    "            r_.append(torch.Tensor([reward]))\n",
    "            v_.append(value)\n",
    "            lp_.append(log_prob)\n",
    "            dones.append(torch.Tensor([not done]))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        eps += 1\n",
    "        batch_mean = (batch_mean*(eps-1)+running_reward)/eps\n",
    "    int_avg = (int_avg*(int_counter-1)+batch_mean)/int_counter\n",
    "    int_counter += 1\n",
    "    avg = (avg*(ep-1)+batch_mean)/ep\n",
    "    trajectory = {\n",
    "                \"rewards\": r_,\n",
    "                \"dones\": dones,\n",
    "                \"values\": v_,\n",
    "                \"log_probs\": lp_}\n",
    "    for _ in range(epochs):\n",
    "        agent.train(trajectory, optimizer)   \n",
    "    if ep % log_interval == 0:\n",
    "        print('Episode {}\\t Interval average: {:.3f}\\t Average reward: {:.3f}'.format(ep, int_avg, avg))\n",
    "        int_avg = 0\n",
    "        int_counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent has no trouble learning this simple task. Though policy gradients tend to be less sample-efficient than value-based methods, they are generally \"simpler\" to get running, and have fewer moving parts. They also have the advantage of being applicable to both discrete and continuous action spaces, whereas value-based methods typically only work for discrete actions. This has changed recently with the advent of pathwise derivative methods such as DDPG, SVG(0) and SAC, though these methods typically act as a blend between the two methods. \n",
    "\n",
    "We can improve the performance of the above algorithm by using an actor-critic framework, in which the policy is an actor that takes actions, and a critic is a value network that tells the agent how good the action is. Traditional actor-critics are online, but we can use the concept as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(Policy):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, gamma=0.99):\n",
    "        super(ActorCritic, self).__init__(input_dim, hidden_dim, output_dim, gamma=0.99)\n",
    "        self.critic = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(input_dim, hidden_dim),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(hidden_dim, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.l1(x))\n",
    "        val = self.value(x)\n",
    "        return F.softmax(self.output(x)), val\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        scores, val = self.forward(x)\n",
    "        dist = Categorical(scores)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action)\n",
    "        return action, val\n",
    "    \n",
    "    def train(self, trajectory, optimizer):\n",
    "        states = torch.stack(trajectory[\"states\"])\n",
    "        actions = torch.stack(trajectory[\"actions\"])\n",
    "        log_probs = torch.stack(trajectory[\"log_probs\"])\n",
    "        next_states = torch.stack(trajectory[\"next_states\"])\n",
    "        rewards = torch.stack(trajectory[\"rewards\"])\n",
    "        values = torch.stack(trajectory[\"values\"])\n",
    "        dones = torch.stack(trajectory[\"dones\"])\n",
    "        \n",
    "        q = 0\n",
    "        for i in reversed(range(rewards.size(dim=0))):\n",
    "            q = rewards[i]+self.gamma*q*dones[i]\n",
    "        \n",
    "        deltas = q-values\n",
    "        actor_loss = -log_probs*deltas\n",
    "        optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
